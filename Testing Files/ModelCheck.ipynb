{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "iyZAxc_DHOo4",
    "outputId": "92841cd1-cf1a-4708-ffef-00a748d58a69"
   },
   "outputs": [],
   "source": [
    "from subprocess import run, PIPE\n",
    "import os\n",
    "# from flask import logging, Flask, render_template, request\n",
    "import logging\n",
    "import random\n",
    "import librosa\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras.activations import relu\n",
    "# import soundfile\n",
    "import soundfile as sf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from spell_correction import correction\n",
    "\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import math\n",
    "# import kenlm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declearation of our Urdu Vocablary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "VBVziHuAH8CZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "char_map_str = \"\"\"\n",
    "<SPACE> 0\n",
    "ا 1\n",
    "آ 2\n",
    "ب 3\n",
    "پ 4\n",
    "ت 5\n",
    "ٹ 6\n",
    "ث 7\n",
    "ج 8\n",
    "چ 9\n",
    "ح 10\n",
    "خ 11\n",
    "د 12\n",
    "ڈ 13\n",
    "ذ 14\n",
    "ر 15\n",
    "ڑ 16\n",
    "ز 17\n",
    "ژ 18\n",
    "س 19\n",
    "ش 20\n",
    "ص 21\n",
    "ض 22\n",
    "ط 23\n",
    "ظ 24\n",
    "ع 25\n",
    "غ 26\n",
    "ف 27\n",
    "ق 28\n",
    "ک 29\n",
    "گ 30\n",
    "ل 31\n",
    "م 32\n",
    "ن 33\n",
    "ں 34\n",
    "و 35\n",
    "ہ 36\n",
    "ھ 37\n",
    "38 ء\n",
    "ئ 39\n",
    "ی 40\n",
    "ے  41\n",
    "'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "\n",
    "for line in char_map_str.strip().split('\\n')[0:-1]:\n",
    "    try:\n",
    "        ch, index = line.split()\n",
    "    #     print('ch ',ch,' index ',index)\n",
    "        char_map[ch.strip()] = int(index)\n",
    "        index_map[int(index)] = ch.strip()\n",
    "    except:\n",
    "        # print('ch ',ch,' index ',index)\n",
    "        char_map[index.strip()] = int(ch) \n",
    "        index_map[int(ch)] = index.strip()\n",
    "       \n",
    "#         break\n",
    "\n",
    "index_map[0] = ' '\n",
    "\n",
    "def int_to_text_sequence(seq):\n",
    "   \n",
    "    text_sequence = []\n",
    "    for c in seq:\n",
    "        if c == 42: #ctc/pad char\n",
    "            ch = ''\n",
    "        else:\n",
    "            ch = index_map[c]\n",
    "        text_sequence.append(ch)\n",
    "    return text_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions to extract audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "JBrc7A1UID58"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clipped_relu(x):\n",
    "    return relu(x, max_value=20)\n",
    "def make_specto_shape(filename, padlen=778):\n",
    "    r = spectrogram_from_file(filename)\n",
    "    print(\"shape of specto \",r.shape)\n",
    "    t = np.transpose(r)  # 2D array ->  spec x timesamples\n",
    "    X = pad_sequences(t, maxlen=padlen, dtype='float', padding='post', truncating='post').T\n",
    "\n",
    "    return X  # MAXtimesamples x specto {max x 161}\n",
    "def get_max_specto_time(filename):\n",
    "    r = spectrogram_from_file(filename)\n",
    "    # print(r.shape)\n",
    "    return r.shape[0]  #\n",
    "def spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for a real signal.\n",
    "    The parameters follow the naming convention of\n",
    "    matplotlib.mlab.specgram\n",
    "\n",
    "    Args:\n",
    "        samples (1D array): input audio signal\n",
    "        fft_length (int): number of elements in fft window\n",
    "        sample_rate (scalar): sample rate\n",
    "        hop_length (int): hop length (relative offset between neighboring\n",
    "            fft windows).\n",
    "\n",
    "    Returns:\n",
    "        x (2D array): spectrogram [frequency x time]\n",
    "        freq (1D array): frequency of each row in x\n",
    "\n",
    "    Note:\n",
    "        This is a truncating computation e.g. if fft_length=10,\n",
    "        hop_length=5 and the signal has 23 elements, then the\n",
    "        last 3 elements will be truncated.\n",
    "    \"\"\"\n",
    "    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n",
    "\n",
    "    window = np.hanning(fft_length)[:, None]\n",
    "    window_norm = np.sum(window**2)\n",
    "\n",
    "    # The scaling below follows the convention of\n",
    "    # matplotlib.mlab.specgram which is the same as\n",
    "    # matlabs specgram.\n",
    "    scale = window_norm * sample_rate\n",
    "\n",
    "    trunc = (len(samples) - fft_length) % hop_length\n",
    "    x = samples[:len(samples) - trunc]\n",
    "\n",
    "    # \"stride trick\" reshape to include overlap\n",
    "    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n",
    "    nstrides = (x.strides[0], x.strides[0] * hop_length)\n",
    "    x = as_strided(x, shape=nshape, strides=nstrides)\n",
    "\n",
    "    # window stride sanity check\n",
    "    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n",
    "\n",
    "    # broadcast window, compute fft over columns and square mod\n",
    "    x = np.fft.rfft(x * window, axis=0)\n",
    "    x = np.absolute(x)**2\n",
    "\n",
    "    # scale, 2.0 for everything except dc and fft_length/2\n",
    "    x[1:-1, :] *= (2.0 / scale)\n",
    "    x[(0, -1), :] /= scale\n",
    "\n",
    "    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n",
    "\n",
    "    return x, freqs\n",
    "\n",
    "def spectrogram_from_array(audio,sample_rate, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "#     with soundfile.SoundFile(filename) as sound_file:\n",
    "#     audio = sound_file.read(dtype='float32')\n",
    "#     audio,sample_rate = librosa.load(filename,sr=16000)\n",
    "#     sample_rate = sound_file.samplerate\n",
    "    if audio.ndim >= 2:\n",
    "        audio = np.mean(audio, 1)\n",
    "    if max_freq is None:\n",
    "        max_freq = sample_rate / 2\n",
    "    if max_freq > sample_rate / 2:\n",
    "        raise ValueError(\"max_freq must not be greater than half of \"\n",
    "                         \" sample rate\")\n",
    "    if step > window:\n",
    "        raise ValueError(\"step size must not be greater than window size\")\n",
    "    hop_length = int(0.001 * step * sample_rate)\n",
    "    fft_length = int(0.001 * window * sample_rate)\n",
    "    pxx, freqs = spectrogram(\n",
    "        audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "        hop_length=hop_length)\n",
    "    ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))\n",
    "\n",
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "#     with soundfile.SoundFile(filename) as sound_file:\n",
    "#     audio = sound_file.read(dtype='float32')\n",
    "    audio,sample_rate = librosa.load(filename,sr=None)\n",
    "    audio = librosa.resample(audio,sample_rate,16000)\n",
    "    sample_rate=16000\n",
    "#     sample_rate = sound_file.samplerate\n",
    "    if audio.ndim >= 2:\n",
    "        audio = np.mean(audio, 1)\n",
    "    if max_freq is None:\n",
    "        max_freq = sample_rate / 2\n",
    "    if max_freq > sample_rate / 2:\n",
    "        raise ValueError(\"max_freq must not be greater than half of \"\n",
    "                         \" sample rate\")\n",
    "    if step > window:\n",
    "        raise ValueError(\"step size must not be greater than window size\")\n",
    "    hop_length = int(0.001 * step * sample_rate)\n",
    "    fft_length = int(0.001 * window * sample_rate)\n",
    "    pxx, freqs = spectrogram(\n",
    "        audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "        hop_length=hop_length)\n",
    "    ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))\n",
    "\n",
    "def model_input(filename):\n",
    "    x_val = [get_max_specto_time(filename)]\n",
    "    max_val = max(x_val)\n",
    "    # print(\"Max batch time value is:\", max_val)\n",
    "    X_data = np.array([make_specto_shape(filename, padlen=max_val)])\n",
    "    with session.as_default():\n",
    "        with session.graph.as_default():\n",
    "            output = report([X_data])[0]\n",
    "    #for i in output:\n",
    "#     out = output[i]\n",
    "    best = list(np.argmax(output[0], axis=1))\n",
    "\n",
    "    #     if merge_chars:\n",
    "    merge = [k for k,g in itertools.groupby(best)]\n",
    "    \n",
    "    #print#()\n",
    "    return \"\".join(int_to_text_sequence(merge))\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    device_count={'GPU': 1},\n",
    "    intra_op_parallelism_threads=1,\n",
    "    allow_soft_placement=True\n",
    ")\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "K.set_session(session)\n",
    "                        \n",
    "path='/home/baeg/Desktop/SpeakCv/Latest_Model/model'\n",
    "jsonfilename = path+\".json\"\n",
    "weightsfilename = path+\".h5\"\n",
    "\n",
    "json_file = open(jsonfilename, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "# K.set_learning_phase(1)\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json,custom_objects={\"clipped_relu\": clipped_relu})\n",
    "\n",
    "# load weights into loaded model\n",
    "loaded_model.load_weights(weightsfilename)\n",
    "loaded_model._make_predict_function()\n",
    "y_pred = loaded_model.get_layer('ctc').input[0]\n",
    "# print('done1')\n",
    "input_data = loaded_model.get_layer('the_input').input\n",
    "# print('done2')\n",
    "K.set_learning_phase(0)\n",
    "# print('done3')\n",
    "report = K.function([input_data, K.learning_phase()], [y_pred])\n",
    "# print('done4')\n",
    "\n",
    "# def audio():\n",
    "#     n = random.randint(0,9)\n",
    "#     with open('tmp/audio'+str(n)+'.wav', 'wb') as f:\n",
    "#         f.write(request.data)\n",
    "    \n",
    "#     #audio,sample_rate=librosa.load('tmp/audio'+str(n)'.wav',sr=16000)\n",
    "#     print('saved as tmp/audio'+str(n)+'.wav')\n",
    "#     #proc = run(['ffprobe', '-of', 'default=noprint_wrappers=1', 'tmp/audio'+str(n)+'.wav'],stderr=PIPE)\n",
    "#     transcript=model_input('tmp/audio'+str(n)+'.wav')\n",
    "#     #clean = run(['ffmpeg -i tmp/audio.wav -af \"highpass=f=200, lowpass=f=3000\" tmp/audio-clean.wav'],stderr=PIPE)\n",
    "\n",
    "#     return transcript\n",
    "\n",
    "def wavCorrector():\n",
    "    fileNames=['name','age',\"address\",\"phone\",\"education\",\"year\",\"profession\",\"experience\"]\n",
    "    fileCount= len(fileNames)\n",
    "    i=0\n",
    "    for f in fileNames:\n",
    "        i+=1\n",
    "        loc=\"/home/baeg/Desktop/SpeakCv/Audios/recordedAudio (\"+str(i)+\").wav\"\n",
    "        if (os.path.exists(loc)==False):\n",
    "            return False\n",
    "        path=os.path.join(loc)\n",
    "        x,_ = librosa.load(path, sr=16000)\n",
    "        path=os.path.join('/home/baeg/Desktop/SpeakCv/Audios/', f+'.wav')\n",
    "        os.remove(loc)\n",
    "        sf.write(path, x, 16000) \n",
    "        print(i)\n",
    "        \n",
    "    return True\n",
    "'''\n",
    "if audio exists, correctly named copy, delete previous\n",
    "else break\n",
    "'''\n",
    "wavCorrector()\n",
    "\n",
    "def asr(location):\n",
    "    # ***** Correcting sample rate of .wav files before sending to asr  ***** \n",
    "#     x,_ = librosa.load(location, sr=16000)\n",
    "#     sf.write(location, x, 16000)\n",
    "    \n",
    "    # ***** CALLING ASR TO GET GENERATE TRANSCRIOTS  ***** \n",
    "    transcript = model_input(location)\n",
    "    \n",
    "    print(\"Original:\", transcript)\n",
    "    transcript = correction(transcript)\n",
    "    print(\"LM corrected:\", transcript)\n",
    "    return transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions for keyword correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################## Keyword functions   \n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def replaceUnicode(words):\n",
    "    for i in range(len(words)):\n",
    "           x=words[i].replace('\\u202c', '')\n",
    "           words[i] = x.replace('\\u202b', '')\n",
    "    return words\n",
    "def city_Dist(sentence):\n",
    "    words =[\"کراچی\", \"لاہور\", \"فیصل\", \"آباد\", \"راول\", \"پنڈی\",\"راولپنڈی\", \"گوجرانوالہ\" ,\"پشاور\", \"ملتان\", \"حیدرآباد\" ,\"اسلام آباد\",\"اسلام\", \"آباد\", \"کوئٹہ\" ,\"بہاولپور\" ,\"بہاولپور\", \"سرگودھا\", \"سیالکوٹ\"]\n",
    "    words=replaceUnicode(words)\n",
    "    sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "    sentence\n",
    "    listed = list()\n",
    "    for word in words:\n",
    "        for i in range(len(sentence)):\n",
    "          ed = nltk.edit_distance(word,sentence[i])\n",
    "          listed.append(tuple([word,i, ed]))\n",
    "          listed.sort(key=lambda x:x[2])\n",
    "    nearest=listed[0][0]\n",
    "    index=listed[0][1]\n",
    "    if listed[1][1]-listed[0][1]==1: #abs(\n",
    "            nearest=listed[0][0]+listed[1][0]\n",
    "    return nearest,index \n",
    "\n",
    "def name_Dist(sentence):\n",
    "    words =[\"جواد\", \"حسن\", \"شفق\" ,\"ریاض\" ,\"بھٹی\", \"اعجاز\" ,\"احمد\" ,\"نورین\" ,\"جمیل\" ,\"زینب\",\"عابد\", \"عمیر\" ,\"ارشد\" ,\"مدیحہ\" ,\"عمر\", \"محمد\", \"توقیر\", \"طلحہ\", \"مجاہد\" ,\"حمزہ\" ,\"علی\" ,\"علینہ\", \"بیگ\" ,\"جواد\", \"حسن\", \"شفق\" ,\"ریاض\" ,\"بھٹی\" ,\"اعجاز\" ,\"احمد\", \"نورین\" ,\"جمیل\", \"زینب\" ,\"عابد\", \"عمیر\", \"ارشد\", \"مدیحہ\" ,\"عمر\", \"محمد\", \"توقیر\" ,\"طلحہ\", \"مجاہد\",\"حمزہ\" ,\"علی\" ,\"علینہ\" ,\"بیگ\" ,\"رباب\", \"عباس\" , \"ادریس\" ,\"خان\" ,\"سونیا\", \"عدنان\", \"کاشف\", \"غلام\", \"قادر\", \"دعا\" ,\"ساجدہ\", \"عزیر\" ,\"ہما\", \"شریف\", \"شوکت\", \"ماہین\", \"الف\", \"خان\" ,\"عبد\" , \"عتیق\", \"الرحمن\" ,\"شان\", \"لائبہ\", \"رضوان\", \"مرتضیٰ\", \"عدینہ\",\"حنا\", \"زین\", \"مناہیل\", \"نور\", \"زرک\" ,\"آمنہ\" ,\"سدرہ\", \"ہنزلہ\" ,\"الیشبہ\" ,\"روہیل\" ,\"شیزہ\", \"فرخ\", \"صائمہ\" ,\"آیان\" ,\"سحر\", \"شازیب\", \"رؤف\", \"شاہ\", \"مدیحہ\", \"جمشید\" ,\"عریج\",\n",
    "    \"علی\" ,\"حسنین\", \"نعمان\", \"جاوید\" ,\"منال\" ,\"اروبا\", \"لقمان\" ,\"ردا\", \"عاصم\" ,\"خدیجہ\" ,\"مدثر\", \"مریم\" ,\"رحیم\",\"سرمد\" ,\"منیب\" ,\"نائلہ\" ,\"زوہیب\",\"سارہ\" ,\"شہر\", \"یار\" ,\"اینی\", \"معاذ\", \"کشاف\" ,\"گل\", \"شیر\" ,\"ہمنا\", \"عاقب\", \"امان\" ,\"موحد\" ,\"فضل\", \"ایسان\" ,\"طارق\" ,\"فہد\",\"توقیر\", \"ضیا\", \"مصطفیٰ\" ,\"خواجہ\", \"شارق\", \"محمد\", \"خان\", \"شہزاد\", \"حسام\", \"ولید\", \"عقبال\", \"ابرار\", \"حسنین\", \"وقار\", \"فیصل\" ,\"آصف\", \"فاطمہ\",\n",
    "    \"زینب\" \"احمد\", \"عثمان\", \"انم\" , \"شیخ\" ,\"ماہرخ\" ,\"بلال\" ,\"رمشا\" ,\"زرش\", \"وقاص\" ,\"صدف\" ,\"راشد\", \"علیا\" ,\"سلمان\", \"آمنہ\", \"ذیشان\" ,\"تابندہ\" ,\"سعد\", \"ابراہیم\" ,\"زاہد\" ,\"راجہ\", \"آمنہ\" ,\"عائشہ\", \"اسماعیل\" ,\"حماد\" ,\"سیف\" ,\"احمد\" ,\n",
    "    \"یاسر\" ,\"مزمل\" ,\"ملک\" ,\"ایشال\"  ,\"وسیم\" ,\"عامر\" ,\"رابعہ\" ,\"حسین\" ,\"سہیل\" ,\"شیراز\" ,\"عائزہ\" ,\"عامر\", \"عنایہ\" ,\"کامران\", \"فزا\"]\n",
    "\n",
    "    words=replaceUnicode(words)\n",
    "    listed = list()\n",
    "    for word in words:\n",
    "#         for i in range(len(sentence)):\n",
    "          ed = similar(word,sentence)\n",
    "          listed.append(tuple([word,sentence, ed]))\n",
    "          listed.sort(key=lambda x:x[2] , reverse=True)\n",
    "    nearest=listed[0][0]\n",
    "    return nearest\n",
    "def set_name(sentence):\n",
    "  tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "  sentence=tokens\n",
    "  name=[]\n",
    "  for i in range(len(tokens)):\n",
    "    if (tokens[i]=='میرا'):\n",
    "      i+=1\n",
    "      if (tokens[i]=='نام'):\n",
    "        i+=1\n",
    "        name=tokens[i:]\n",
    "        for n in range(i,len(tokens)):\n",
    "            if (sentence[i]!=\"ہے\"):\n",
    "                x=name_Dist(sentence[i])\n",
    "                sentence[i]=x\n",
    "            i+=1\n",
    "    elif (tokens[i]=='میں'):\n",
    "      i+=1\n",
    "      name=tokens[i:]\n",
    "      for n in range(i,len(tokens)):\n",
    "            if (sentence[i]!=\"ہو\" and sentence[i]!=\"ہوں\" ):\n",
    "                x=name_Dist(sentence[i])\n",
    "                sentence[i]=x \n",
    "  str1=\" \"\n",
    "  if len(name)==0:\n",
    "    str1='None'\n",
    "  else:\n",
    "    str1=str1.join(sentence)\n",
    "    \n",
    "  return str1\n",
    "\n",
    "def set_Address(s):\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    sentence=tokens\n",
    "    words =[\"میرا‬\",\"تعلق‬\",\"سے‬\",\"ہے‬\",\"رہتا‬\",\"ہوں\",\"ہو\",\"میں\",\"کراچی\", \"لاہور\", \"فیصل\", \"آباد\", \"راول\", \"پنڈی\",\"راولپنڈی\", \"گوجرانوالہ\" ,\"پشاور\", \"ملتان\", \"حیدرآباد\" ,\"اسلام آباد\",\"اسلام\", \"آباد\", \"کوئٹہ\" ,\"بہاولپور\" ,\"بہاولپور\", \"سرگودھا\", \"سیالکوٹ\"]\n",
    "    words=replaceUnicode(words)\n",
    "    for i  in range(len(tokens)):\n",
    "        listed = list()\n",
    "        for word in words:\n",
    "    #         for i in range(len(sentence)):\n",
    "              ed = similar(word,sentence[i])\n",
    "              listed.append(tuple([word,sentence[i], ed]))\n",
    "              listed.sort(key=lambda x:x[2] , reverse=True)\n",
    "        nearest=listed[0][0]\n",
    "        sentence[i]=nearest\n",
    "    str1=\" \"\n",
    "    if len(sentence)==0:\n",
    "        str1='None'\n",
    "    else:\n",
    "        str1=str1.join(sentence)\n",
    "    return str1\n",
    "def set_Phone(s):\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    sentence=tokens\n",
    "    words = [\"صفر\",\"ایک\",\"دو\",\"تین\",\"چار\",\"پانچ\",\"چھ\",\"سات\",\"آٹھ\",\"نو\",\"میرا\",\"فون\" ,\"نمبر\",\"زیرو\",\"ون\", \"ٹو\" ,\"تھری\" ,\"فور\", \"فائیو\" ,\"سکس\" ,\"سیون\", \"آٹھ\", \"نائن\",\"ہے\", \"یہ\"]\n",
    "\n",
    "    words=replaceUnicode(words)\n",
    "    for i  in range(len(tokens)):\n",
    "        listed = list()\n",
    "        for word in words:\n",
    "    #         for i in range(len(sentence)):\n",
    "              ed = similar(word,sentence[i])\n",
    "              listed.append(tuple([word,sentence[i], ed]))\n",
    "              listed.sort(key=lambda x:x[2] , reverse=True)\n",
    "        nearest=listed[0][0]\n",
    "        sentence[i]=nearest\n",
    "    str1=\" \"\n",
    "    if len(sentence)==0:\n",
    "        str1='None'\n",
    "    else:\n",
    "        str1=str1.join(sentence)\n",
    "    return str1\n",
    "\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErRgMtH5ME7j"
   },
   "source": [
    "### Defining functions for keyword extraction and CV generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/baeg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/baeg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "########\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "##########\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from google_trans_new import google_translator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "QOQ9kvjxMEMV"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    is_number , text2int functions are used to convert number words to numbers\n",
    "    taken from https://stackoverflow.com/questions/493174/is-there-a-way-to-convert-number-words-to-integers\n",
    "'''\n",
    "def is_number(x):\n",
    "    if type(x) == str:\n",
    "        x = x.replace(',', '')\n",
    "    try:\n",
    "        float(x)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text2int (textnum, numwords={}):\n",
    "    units = [\n",
    "        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',\n",
    "        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',\n",
    "        'sixteen', 'seventeen', 'eighteen', 'nineteen',\n",
    "    ]\n",
    "    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']\n",
    "    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']\n",
    "    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}\n",
    "    ordinal_endings = [('ieth', 'y'), ('th', '')]\n",
    "\n",
    "    if not numwords:\n",
    "        numwords['and'] = (1, 0)\n",
    "        for idx, word in enumerate(units): numwords[word] = (1, idx)\n",
    "        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)\n",
    "        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "    textnum = textnum.replace('-', ' ')\n",
    "\n",
    "    current = result = 0\n",
    "    curstring = ''\n",
    "    onnumber = False\n",
    "    lastunit = False\n",
    "    lastscale = False\n",
    "\n",
    "    def is_numword(x):\n",
    "        if is_number(x):\n",
    "            return True\n",
    "        if word in numwords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def from_numword(x):\n",
    "        if is_number(x):\n",
    "            scale = 0\n",
    "            increment = int(x.replace(',', ''))\n",
    "            return scale, increment\n",
    "        return numwords[x]\n",
    "\n",
    "    for word in textnum.split():\n",
    "        if word in ordinal_words:\n",
    "            scale, increment = (1, ordinal_words[word])\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "            onnumber = True\n",
    "            lastunit = False\n",
    "            lastscale = False\n",
    "        else:\n",
    "            for ending, replacement in ordinal_endings:\n",
    "                if word.endswith(ending):\n",
    "                    word = \"%s%s\" % (word[:-len(ending)], replacement)\n",
    "\n",
    "            if (not is_numword(word)) or (word == 'and' and not lastscale):\n",
    "                if onnumber:\n",
    "                    # Flush the current number we are building\n",
    "                    curstring += repr(result + current) + \" \"\n",
    "                curstring += word + \" \"\n",
    "                result = current = 0\n",
    "                onnumber = False\n",
    "                lastunit = False\n",
    "                lastscale = False\n",
    "            else:\n",
    "                scale, increment = from_numword(word)\n",
    "                onnumber = True\n",
    "\n",
    "                if lastunit and (word not in scales):                                                                                                                                                                                                                                         \n",
    "                    # Assume this is part of a string of individual numbers to                                                                                                                                                                                                                \n",
    "                    # be flushed, such as a zipcode \"one two three four five\"                                                                                                                                                                                                                 \n",
    "                    curstring += repr(result + current)                                                                                                                                                                                                                                       \n",
    "                    result = current = 0                                                                                                                                                                                                                                                      \n",
    "\n",
    "                if scale > 1:                                                                                                                                                                                                                                                                 \n",
    "                    current = max(1, current)                                                                                                                                                                                                                                                 \n",
    "\n",
    "                current = current * scale + increment                                                                                                                                                                                                                                         \n",
    "                if scale > 100:                                                                                                                                                                                                                                                               \n",
    "                    result += current                                                                                                                                                                                                                                                         \n",
    "                    current = 0                                                                                                                                                                                                                                                               \n",
    "\n",
    "                lastscale = False                                                                                                                                                                                                              \n",
    "                lastunit = False                                                                                                                                                \n",
    "                if word in scales:                                                                                                                                                                                                             \n",
    "                    lastscale = True                                                                                                                                                                                                         \n",
    "                elif word in units:                                                                                                                                                                                                             \n",
    "                    lastunit = True\n",
    "\n",
    "    if onnumber:\n",
    "        curstring += repr(result + current)\n",
    "\n",
    "    return curstring\n",
    "#################################### Keyword extraction functions ############################################\n",
    "def get_name(sentence):\n",
    "  tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "  name=[]\n",
    "  for i in range(len(tokens)):\n",
    "    #my name is\n",
    "    if (tokens[i].lower()=='name'):\n",
    "      i+=1\n",
    "      if (tokens[i].lower()=='is'):\n",
    "        i+=1\n",
    "      name=tokens[i:]\n",
    "    #I am hanan\n",
    "    elif (tokens[i].lower()=='i'):\n",
    "      i+=1\n",
    "      if (tokens[i].lower()=='am'):\n",
    "        i+=1\n",
    "      name=tokens[i:]\n",
    "  name[:2]\n",
    "  str1=\" \"\n",
    "  if len(name)==0:\n",
    "    str1='None'\n",
    "  else:\n",
    "    str1=str1.join(name)\n",
    "  return str1\n",
    "\n",
    "\n",
    "def get_age(sentence):\n",
    "  tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "  age=[]\n",
    "\n",
    "  doc=nlp(sentence)\n",
    "  for X in doc.ents:\n",
    "    if(X.label_=='DATE'):\n",
    "      age.append(X.text)\n",
    "\n",
    "  if(len(age)==0):\n",
    "    age=['None']\n",
    "    \n",
    "  str1=\" \"\n",
    "  if len(age)==0:\n",
    "    str1='None'\n",
    "  else:\n",
    "    str1=str1.join(age)\n",
    "  return str1\n",
    "\n",
    "def get_phone(sentence):\n",
    "  num = \"\"\n",
    "  for c in sentence:\n",
    "      if c.isdigit():\n",
    "          num = num + c\n",
    "  if num==\"\":\n",
    "    num='03311234567'\n",
    "  return num\n",
    "\n",
    "\n",
    "def get_address(sentence):\n",
    "  tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "    #what if users says something diff like mera ghar karachi main hain lekin islamabad main kaam kr raha hun  (\"I live in Rawalpindi\")\n",
    "    #POS tagger , NER\n",
    "  addr=[]\n",
    "  for i in range(len(tokens)):\n",
    "\n",
    "    #I live in Faisalabad\n",
    "    if (tokens[i].lower()=='live'):\n",
    "      i+=1\n",
    "      if (tokens[i].lower()=='in'):\n",
    "        i+=1\n",
    "      addr=tokens[i:]\n",
    "\n",
    "    #I belong to \n",
    "    elif (tokens[i].lower()=='belong'):\n",
    "      i+=1\n",
    "      if (tokens[i].lower()=='to'):\n",
    "        i+=1\n",
    "      addr=tokens[i:]\n",
    "\n",
    "    #I am from \n",
    "    elif (tokens[i].lower()=='from'):\n",
    "      i+=1\n",
    "      addr=tokens[i:]\n",
    "    else:\n",
    "      addr=tokens[i:]\n",
    "\n",
    "  addr[:2]\n",
    "  str1=\" \"\n",
    "  if len(addr)==0:\n",
    "    str1='None'\n",
    "  else:\n",
    "    str1=str1.join(addr)\n",
    "  return str1\n",
    "\n",
    "\n",
    "def get_education(sentence):\n",
    "  year=''\n",
    "  s=text2int(sentence) +' '     #convert year in words to number                                  \n",
    "  yr=re.findall(\"(\\d{4})\\D\",s)\n",
    "  if (len(yr)!=0):      # IF THERE is any year mentioned, get it\n",
    "    year=yr[0]\n",
    "\n",
    "  tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "  EDUCATION = ['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th','11th','12th',\n",
    "              'first','second','third','fourth','fifth','sixth','seventh','eight','ninth','tenth','elevnth','twelfth',\n",
    "              'inter','middle','intermediate','matriculation','matric','ICS','Diploma','FSC','FA',\n",
    "              'BA','BCom','B.Com',  'BTECH', 'B.TECH' , 'graduate', 'SSC', 'HSC' ,'illiterate' ]\n",
    "  edu=[]\n",
    "  for i in range(len(tokens)):\n",
    "    x=tokens[i].lower()\n",
    "    for i in range(len(EDUCATION)):\n",
    "      c=EDUCATION[i].lower()\n",
    "      if (nltk.edit_distance(c , x )<=1):\n",
    "          if (i<24):\n",
    "            c+=\" grade\"\n",
    "          edu.append(c)\n",
    "          break\n",
    "  if len(edu)==0:\n",
    "    str1='None'\n",
    "  else:\n",
    "    str1=str(edu[0])\n",
    "  return str1\n",
    "\n",
    "def get_year(sentence):\n",
    "    year=''\n",
    "    s=text2int(sentence) +' '     #convert year in words to number                                  \n",
    "    yr=re.findall(\"(\\d{4})\\D\",s)\n",
    "    if (len(yr)!=0):      # IF THERE is any year mentioned, get it\n",
    "         year=str(max(yr))\n",
    "\n",
    "    return year\n",
    "\n",
    "\n",
    "def get_profession(sentence):\n",
    "  tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "  proffession=[\"delivery\",\"carpenter\", \"driver\",\"electrician\",\"plumber\"]\n",
    "  #\"I know plumber work\"\n",
    "  #\"I am a carpenter\"\n",
    "  prof=[]\n",
    "  for i in range(len(tokens)):\n",
    "    #I live in Faisalabad\n",
    "    x=tokens[i].lower()\n",
    "    for c in proffession:\n",
    "      if (nltk.edit_distance(c , x )<3):\n",
    "          if (c==\"delivery\"):\n",
    "            c=\"delivery boy\"\n",
    "          prof.append(c)\n",
    "  str1=\" \"\n",
    "  if len(prof)==0:\n",
    "    str1='None'\n",
    "  else:\n",
    "    str1=str1.join(prof) \n",
    "  return str1\n",
    "\n",
    "\n",
    "def get_experience(line):\n",
    "  tokens = nltk.tokenize.word_tokenize(line)\n",
    "  exp=[]\n",
    "  doc=nlp(line)\n",
    "  for X in doc.ents:\n",
    "    if(X.label_=='DATE'):\n",
    "      exp.append(X.text)\n",
    "\n",
    "  if(len(exp)==0):\n",
    "    exp=['None']\n",
    "    \n",
    "  str1=\" \"\n",
    "  if len(exp)==0:\n",
    "    str1='None'\n",
    "  else:\n",
    "    str1=str1.join(exp) \n",
    "  \n",
    "  return str1\n",
    "\n",
    "\n",
    "#################################### Keyword placement function ############################################\n",
    "from docx import Document\n",
    "def Place_Keywords(arr,path):\n",
    "  document = Document(path)\n",
    "  for paragraph in document.paragraphs:\n",
    "      if 'nm_x' in paragraph.text:\n",
    "          #name,age,address,phone,education, year,profession,experience\n",
    "          #0    1    2        3      4        5     6         7\n",
    "          paragraph.text = arr[0]\n",
    "      elif 'ad_x' in paragraph.text:\n",
    "          paragraph.text = arr[2]\n",
    "      elif 'ph_x' in paragraph.text:\n",
    "          paragraph.text = arr[3]\n",
    "      elif 'ag_x' in paragraph.text:\n",
    "          paragraph.text = arr[1]\n",
    "      elif 'Prof_x' in paragraph.text:\n",
    "          paragraph.text = arr[6]\n",
    "      elif 'exp_x' in paragraph.text:\n",
    "          paragraph.text = arr[7]\n",
    "      elif 'ed_x' in paragraph.text:\n",
    "          # print(paragraph.text)\n",
    "          paragraph.text = arr[4]\n",
    "      elif 'Yr_x' in paragraph.text:\n",
    "          paragraph.text = arr[5]\n",
    "      document.save('/home/baeg/Desktop/SpeakCv/CV/MY_CV.docx')\n",
    "  return\n",
    "\n",
    "def ifNone(str):\n",
    "    if len(str)==0:\n",
    "        return 'None'\n",
    "    else:\n",
    "        return str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOBgrfLk1vnq"
   },
   "source": [
    "# GenerateCV function does the following tasks\n",
    "\n",
    "*   generates text\n",
    "*   Translates it\n",
    "*   Returns Keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "_CdCP_XEVnh8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def GenerateCV(transcripts):\n",
    "#   print(\"\\n****** Transcript ******\\n\",transcripts[0],transcripts[1],transcripts[2],transcripts[3],transcripts[4],transcripts[5],transcripts[6],transcripts[7],sep='\\n')\n",
    "  transcripts[0]=set_name(transcripts[0])\n",
    "  print(\"\\nName Corrected:\",transcripts[0])\n",
    "  transcripts[2]=set_Address(transcripts[2])\n",
    "  print(\"Address Corrected:\",transcripts[2])\n",
    "    \n",
    "  #************************ TRANSLATION  ********************************\n",
    "  translator = google_translator()  \n",
    "  eng=[]\n",
    "  for s in transcripts:\n",
    "    translation = translator.translate(s,lang_tgt='en')  \n",
    "    eng.append(translation)\n",
    "  # i=0\n",
    "  # for s in transcripts:\n",
    "  #   print(s,\" \" ,eng[i] )\n",
    "  #   i+=1\n",
    "\n",
    "  #************************ Keyword Extraction  ********************************\n",
    "# name-0 age-1 address-2 phone-3 edu-4 year-5 prof-6 exp-7\n",
    "#   print(transcripts,eng)\n",
    "  name=eng[0]\n",
    "  age=eng[1]\n",
    "  address=eng[2]\n",
    "  phone=eng[3]\n",
    "  education=eng[4]\n",
    "  year=eng[5]\n",
    "  profession=eng[6]\n",
    "  experience=eng[7] \n",
    "\n",
    "  print(\"\\n****** Translation ******\\n\",name,age,address,phone,education,year,profession,experience,sep='\\n')\n",
    "  \n",
    "  keywords=[]\n",
    "# name-0 age-1 address-2 phone-3 edu-4 year-5 prof-6 exp-7\n",
    "  keywords.append(get_name(name))\n",
    "  keywords.append(get_age(age))\n",
    "  keywords.append(get_address(address))\n",
    "  keywords.append(get_phone(phone))\n",
    "  keywords.append(get_education(education))\n",
    "  keywords.append(get_year(year))\n",
    "  keywords.append(get_profession(profession))\n",
    "  keywords.append(get_experience(experience))\n",
    "  \n",
    "  print(\"\\n****** Keywords ******\\n\",name,age,address,phone,education,year,profession,experience,sep='\\n')\n",
    "  cv_path=\"/home/baeg/Desktop/SpeakCv/cv_template.docx\"\n",
    "  #print(name,age,address,phone,education+\" \"+ year,profession,experience,sep='\\n')\n",
    "  Place_Keywords(keywords, cv_path)\n",
    "  return keywords\n",
    "# file_object.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling asr(),and GenerateCV functions to get transcripts from trained model and generate CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baeg/anaconda3/lib/python3.7/site-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
      "/home/baeg/anaconda3/lib/python3.7/site-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of specto  (437, 161)\n",
      "Original: میرا نام سارا ہے\n",
      "LM corrected: میرا نام سارا ہے\n",
      "shape of specto  (419, 161)\n",
      "Original: میری مر چالیس سال ہے\n",
      "LM corrected: میری مر چالیس سال ہے\n",
      "shape of specto  (431, 161)\n",
      "Original: م اسلام باد سے ہوںے\n",
      "LM corrected: م اسلام باد سے ہونے\n",
      "shape of specto  (1091, 161)\n",
      "Original: میرا فور نمبر ہے زیرو تری ون ون زیرو ون ٹو فور ٹری سس\n",
      "LM corrected: میرا فور نمبر ہے زیرو تری ون ون زیرو ون ٹو فور ٹری سے\n",
      "shape of specto  (437, 161)\n",
      "Original: میں نے میٹرک کیا ہے\n",
      "LM corrected: میں نے میٹرک کیا ہے\n",
      "shape of specto  (641, 161)\n",
      "Original: میں نے دو ہزار دس نے ویٹرک کیا تھے\n",
      "LM corrected: میں نے دو ہزار دس نے میٹرک کیا تھے\n",
      "shape of specto  (377, 161)\n",
      "Original: میرہ پیشہ پلمب ہے\n",
      "LM corrected: میرا پیشہ پلمبر ہے\n",
      "shape of specto  (509, 161)\n",
      "Original: میں نے دس سال کام کیا ہے\n",
      "LM corrected: میں نے دس سال کام کیا ہے\n",
      "\n",
      "Name Corrected: میرا نام سارہ ہے\n",
      "Address Corrected: میں اسلام آباد سے ہے\n",
      "\n",
      "****** Translation ******\n",
      "\n",
      "My name is Sarah \n",
      "My dead is forty years \n",
      "I have from Islamabad \n",
      "My For The Number is Zero Tier Win Zero One to Tour Tree \n",
      "I have metric \n",
      "I did two thousand ten metric \n",
      "My profession is plumber \n",
      "I have worked ten years \n",
      "\n",
      "****** Keywords ******\n",
      "\n",
      "My name is Sarah \n",
      "My dead is forty years \n",
      "I have from Islamabad \n",
      "My For The Number is Zero Tier Win Zero One to Tour Tree \n",
      "I have metric \n",
      "I did two thousand ten metric \n",
      "My profession is plumber \n",
      "I have worked ten years \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sarah',\n",
       " 'forty years',\n",
       " 'Islamabad',\n",
       " '03311234567',\n",
       " 'matric',\n",
       " '2010',\n",
       " 'plumber',\n",
       " 'ten years']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileNames=['name','age',\"address\",\"phone\",\"education\",\"year\",\"profession\",\"experience\"]\n",
    "Transcripts=[]\n",
    "loc='/home/baeg/Desktop/SpeakCv/Audios/'\n",
    "wavCorrector()\n",
    "for f in fileNames:\n",
    "    \n",
    "    l=loc+f+\".wav\"\n",
    "    x=asr(l)\n",
    "#     x=ASR_G(l)\n",
    "    Transcripts.append(x)\n",
    "GenerateCV(Transcripts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NAMES\n",
    "# words1=[\"Jawad\", \"Hassan\", \"Shafaq\", \"Riaz\", \"Bhatti\", \"Ejaz\", \"Ahmed\", \"Noreen\", \"Jamil\", \"Zainab\", \"Abaid\", \"Umair\", \"Arshad\", \"Madiha\", \"Umar\", \"Muhammad\", \"Toqeer\", \"talha\", \"mujahid\", \"Hamza\", \"ali\", \"alina\", \"baig\", \"fatima\", \"zainab\", \"Ahmed\", \"usman\", \"Anum\", \"Rana\", \"Sheikh\", \"mahrukh\", \"Bilal\", \"Ramsha\", \"zarish\", \"Waqas\", \"Sadaf\", \"Rashid\", \"ALIYA\", \"Salman\", \"Amna\", \"zeeshan\", \"Tabinda\", \"saad\", \"Ibrahim\", \"Zahid\", \"raja\", \"Amna\", \"Ayesha\", \"ismail\", \"Hammad\", \"saif\", \"Ahmad\", \"Yasir\", \"Muzammil\", \"Malik\", \"Eishal\", \"Hassan\", \"Waseem\", \"Amir\", \"Rabia\", \"Hussain\", \"sohail\", \"Sheraz\", \"Ayeza\", \"Ameer\", \"Inaya\", \"Kamran\", \"Fizza\", \"Rabab\", \"Abbas\", \"Khudija\", \"idrees\", \"khan\", \"sonia\", \"Adnan\", \"kashif\", \"Ghulam\", \"Qadir\", \"Dua\", \"sajida\", \"Uzair\", \"Huma\", \"sharif\", \"shaukat\", \"maheen\", \"alif\", \"khan\", \"Abd\" , \"ur rehman\", \"Ateeq\", \"ul rehman\", \"Shan\", \"Laiba\", \"Rizwan\", \"Murtaza\", \"Adeena\", \"HINA\", \"Zain\", \"Minahil\", \"Noor\", \"Zarak\", \"Amina\", \"Sidra\", \"hanzla\", \"Alishba\", \"Rohail\", \"sheeza\", \"Farrukh\", \"Saima\", \"Ayaan\", \"sahar\", \"Shazeb\", \"Rauf\", \"Shah\", \"Madhia\", \"jamshed\", \"Areej\", \"Ali\", \"Husnain\", \"Numan \", \"javed\", \"Manaal\", \"Aruba\", \"luqman\", \"Rida\", \"asim\", \"Khadija\", \"Mudassir\", \"Maryam\", \"rahim\", \"sarmad\", \"Muneeb\", \"Naila\", \"Zohaib\", \"Sarah\", \"Shaheryar\", \"Annie\", \"Maaz\", \"Kashaf\", \"Gul\", \"Sher\", \"Humna\", \"Aaqib\", \"Amaan\", \"Mohid\", \"fazal\", \"Esan\", \"tariq\", \"Fahad\", \"Toqeer\", \"Zia\", \"mustafa\", \"Khawaja\", \"Shariq\", \"Muhammad\", \"khan\", \"Shahzad\", \"Hassam\", \"Waleed\", \"iqbal\", \"abrar\", \"Hasnain\", \"waqar\", \"faisal\", \"asif\", ]\n",
    "# words =[\"جواد\", \"حسن\", \"شفق\" ,\"ریاض\" ,\"بھٹی\", \"اعجاز\" ,\"احمد\" ,\"نورین\" ,\"جمیل\" ,\"زینب\",\"عابد\", \"عمیر\" ,\"ارشد\" ,\"مدیحہ\" ,\"عمر\", \"محمد\", \"توقیر\", \"طلحہ\", \"مجاہد\" ,\"حمزہ\" ,\"علی\" ,\"علینہ\", \"بیگ\" ,\"جواد\", \"حسن\", \"شفق\" ,\"ریاض\" ,\"بھٹی\" ,\"اعجاز\" ,\"احمد\", \"نورین\" ,\"جمیل\", \"زینب\" ,\"عابد\", \"عمیر\", \"ارشد\", \"مدیحہ\" ,\"عمر\", \"محمد\", \"توقیر\" ,\"طلحہ\", \"مجاہد\",\"حمزہ\" ,\"علی\" ,\"علینہ\" ,\"بیگ\" ,\"رباب\", \"عباس\" , \"ادریس\" ,\"خان\" ,\"سونیا\", \"عدنان\", \"کاشف\", \"غلام\", \"قادر\", \"دعا\" ,\"ساجدہ\", \"عزیر\" ,\"ہما\", \"شریف\", \"شوکت\", \"ماہین\", \"الف\", \"خان\" ,\"عبد\" , \"عتیق\", \"الرحمن\" ,\"شان\", \"لائبہ\", \"رضوان\", \"مرتضیٰ\", \"عدینہ\",\"حنا\", \"زین\", \"مناہیل\", \"نور\", \"زرک\" ,\"آمنہ\" ,\"سدرہ\", \"ہنزلہ\" ,\"الیشبہ\" ,\"روہیل\" ,\"شیزہ\", \"فرخ\", \"صائمہ\" ,\"آیان\" ,\"سحر\", \"شازیب\", \"رؤف\", \"شاہ\", \"مدیحہ\", \"جمشید\" ,\"عریج\",\n",
    "#     \"علی\" ,\"حسنین\", \"نعمان\", \"جاوید\" ,\"منال\" ,\"اروبا\", \"لقمان\" ,\"ردا\", \"عاصم\" ,\"خدیجہ\" ,\"مدثر\", \"مریم\" ,\"رحیم\",\"سرمد\" ,\"منیب\" ,\"نائلہ\" ,\"زوہیب\",\"سارہ\" ,\"شہر\", \"یار\" ,\"اینی\", \"معاذ\", \"کشاف\" ,\"گل\", \"شیر\" ,\"ہمنا\", \"عاقب\", \"امان\" ,\"موحد\" ,\"فضل\", \"ایسان\" ,\"طارق\" ,\"فہد\",\"توقیر\", \"ضیا\", \"مصطفیٰ\" ,\"خواجہ\", \"شارق\", \"محمد\", \"خان\", \"شہزاد\", \"حسام\", \"ولید\", \"عقبال\", \"ابرار\", \"حسنین\", \"وقار\", \"فیصل\" ,\"آصف\", \"فاطمہ\",\n",
    "#     \"زینب\" \"احمد\", \"عثمان\", \"انم\" , \"شیخ\" ,\"ماہرخ\" ,\"بلال\" ,\"رمشا\" ,\"زرش\", \"وقاص\" ,\"صدف\" ,\"راشد\", \"علیا\" ,\"سلمان\", \"آمنہ\", \"ذیشان\" ,\"تابندہ\" ,\"سعد\", \"ابراہیم\" ,\"زاہد\" ,\"راجہ\", \"آمنہ\" ,\"عائشہ\", \"اسماعیل\" ,\"حماد\" ,\"سیف\" ,\"احمد\" ,\n",
    "#     \"یاسر\" ,\"مزمل\" ,\"ملک\" ,\"ایشال\"  ,\"وسیم\" ,\"عامر\" ,\"رابعہ\" ,\"حسین\" ,\"سہیل\" ,\"شیراز\" ,\"عائزہ\" ,\"عامر\", \"عنایہ\" ,\"کامران\", \"فزا\"]\n",
    "# len(words),len(words1)\n",
    "\n",
    "\n",
    "# def city_Dist(sentence):\n",
    "#     words =[\"جواد\", \"حسن\", \"شفق\" ,\"ریاض\" ,\"بھٹی\", \"اعجاز\" ,\"احمد\" ,\"نورین\" ,\"جمیل\" ,\"زینب\"\n",
    "\n",
    "#     ,\"عابد\", \"عمیر\" ,\"ارشد\" ,\"مدیحہ\" ,\"عمر\", \"محمد\", \"توقیر\", \"طلحہ\", \"مجاہد\" ,\"حمزہ\" ,\"علی\"]\n",
    "#     words1=[\"Jawad\", \"Hassan\", \"Shafaq\", \"Riaz\", \"Bhatti\", \"Ejaz\", \"Ahmed\", \"Noreen\", \"Jamil\", \"Zainab\",\n",
    "\n",
    "#     \"Abaid\", \"Umair\", \"Arshad\", \"Madiha\", \"Umar\", \"Muhammad\", \"Toqeer\", \"talha\", \"mujahid\", \"Hamza\", \"ali\"]\n",
    "#     len(words),len(words1)\n",
    "# #     words =[\"کراچی\", \"لاہور\", \"فیصل\", \"آباد\", \"راول\", \"پنڈی\",\"راولپنڈی\", \"گوجرانوالہ\" ,\"پشاور\", \"ملتان\", \"حیدرآباد\" ,\"اسلام آباد\",\"اسلام\", \"آباد\", \"کوئٹہ\" ,\"بہاولپور\" ,\"بہاولپور\", \"سرگودھا\", \"سیالکوٹ\"]\n",
    "#     words=replaceUnicode(words)\n",
    "#     words1=replaceUnicode(words1)\n",
    "#     sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "#     sentence\n",
    "#     listed = list()\n",
    "#     for word in words:\n",
    "#         for i in range(len(sentence)):\n",
    "#           ed = similar(word,sentence[i])\n",
    "#           listed.append(tuple([word,i, ed]))\n",
    "#           listed.sort(key=lambda x:x[2],reverse=True)\n",
    "#     nearest=listed[0][0]\n",
    "#     index=listed[0][1]\n",
    "#     if listed[1][1]-listed[0][1]==1: #abs(\n",
    "#             nearest=listed[0][0]+listed[1][0]\n",
    "#     print(nearest,index,words1[index])\n",
    "#     return nearest,index \n",
    "# sentence=\"میرا نام لینا ہے\"\n",
    "# set_name(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # sentence=\"میرا نام لینا ہے\"\n",
    "# # sentence=\"میں  لینا ہوں\"\n",
    "# # s=\"م اسلام باد سے ہونے\"\n",
    "# # set_name(sentence)\n",
    "# # s=set_Address(s)\n",
    "# s=\"میرا فور نمبر ہے زیرو تری ون ون زیرو ون ٹو فور ٹری سس\"\n",
    "\n",
    "# set_name(sentence)\n",
    "# s=set_Phone(s)\n",
    "# s\n",
    "# translator = google_translator()  \n",
    "# translate_text = translator.translate(s,lang_tgt='en')  \n",
    "# get_phone(translate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "Cq2GDKsN9zMq"
   },
   "outputs": [],
   "source": [
    "# # transcripts=ASR_G()\n",
    "# Transcripts=[\"میرا نام حنان ہے\",\"میری عمر 27 سال ہے\",\"میں راولپنڈی میں رہتا ہوں\",\"میرا فون نمبر یہ ہے 03001234567\",\" میں نے مڈل تک تعلیم حاصل کی ہے\",\"میں نے 2010 میں میٹرک کیا\",\" میرا پیشہ نمبر ہے\",\" میرا پانچ سال کا تجربہ ہے\"]\n",
    "# GenerateCV(Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW5UPelYy1Wq",
    "outputId": "0665baa2-9ba6-4bd5-c5dc-efe1da694cca"
   },
   "outputs": [],
   "source": [
    "# # Calling GenerateCV function here...\n",
    "# location='/home/baeg/Desktop/SpeakCv/Audios/'\n",
    "# keywords=GenerateCV(location)\n",
    "# print(\"\\n\",keywords[0],keywords[1], keywords[2], keywords[3], keywords[4]+\" \"+ keywords[5], keywords[6], keywords[7],sep='\\n')\n",
    "# #       (     name,       age,        address,     phone,     education+\" \"  + year,         profession,  experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "s6Gk01INCF9g"
   },
   "outputs": [],
   "source": [
    "# sentence=\"My For The Number is Zero Tier Win Zero One to Tour Tree\"\n",
    "# sentence1=[]\n",
    "# words = [\"my\" , \"phone\" , \"number\" , \"is\" , \"zero\" , \"one\" , \"two\" , \"three\", \"four\" , \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "# sentence=\"میرا فور نمبر ہے زیرو تری ون ون زیرو ون ٹو فور ٹری سے\"\n",
    "# words = [\"صفر\",\"ایک\",\"دو\",\"تین\",\"چار\",\"پانچ\",\"چھ\",\"سات\",\"آٹھ\",\"نو\",\"میرا\",\"فون\" ,\"نمبر\",\"زیرو\",\"ون\", \"ٹو\" ,\"تھری\" ,\"فور\", \"فائیو\" ,\"سکس\" ,\"سیون\", \"آٹھ\", \"نائن\"]  \n",
    "\n",
    "# words =[\"کراچی\", \"لاہور\", \"فیصل\", \"آباد\", \"راول\", \"پنڈی\",\"راولپنڈی\", \"گوجرانوالہ\" ,\"پشاور\", \"ملتان\", \"حیدرآباد\" ,\"اسلام آباد\",\"اسلام\", \"آباد\", \"کوئٹہ\" ,\"بہاولپور\" ,\"بہاولپور\", \"سرگودھا\", \"سیالکوٹ\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get_address()\n",
    "    # words =[\"‫میں‬\",\"‫سے‬\",\"‫ہے‬\",\"‫تعلق‬\",\"‫رہتا‬\",\"‫ہوں‬\",\"رہتی\",\"‫میرا‬\",           \"کراچی\", \"لاہور\", \"فیصل\", \"آباد\", \"راول\", \"پنڈی\",\"راولپنڈی\", \"گوجرانوالہ\" ,\"پشاور\", \"ملتان\", \"حیدرآباد\" ,\"اسلام آباد\",\"اسلام\", \"آباد\", \"کوئٹہ\" ,\"بہاولپور\" ,\"بہاولپور\", \"سرگودھا\", \"سیالکوٹ\"]\n",
    "    # print(words)\n",
    "# \n",
    "  # words = [\"صفر\",\"ایک\",\"دو\",\"تین\",\"چار\",\"پانچ\",\"چھ\",\"سات\",\"آٹھ\",\"نو\",\"میرا\",\"فون\" ,\"نمبر\",\"زیرو\",\"ون\", \"ٹو\" ,\"تری\" ,\"فور\", \"فائیو\" ,\"سکس\" ,\"سیون\", \"آٹھ\", \"نائن\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #      main,sub \n",
    "# sentence=\"لینا\"\n",
    "# type(name_Dist(sentence))\n",
    "# # print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a=\"زینب\"\n",
    "# b='لینا'\n",
    "# c=\"عمیر\"\n",
    "# d=\"علینہ\"\n",
    "\n",
    "# print(similar(b, a))\n",
    "# print(similar(b, c))\n",
    "# print(similar(b, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city,index=city_Dist(\"م اسلام باد سے ہونے\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM corrected: میرا نام لینا ہے\n",
    "# LM corrected: میری مر چالیس سال ہے\n",
    "# LM corrected: م اسلام باد سے ہونے\n",
    "# LM corrected: میرا فور نمبر ہے زیرو تری ون ون زیرو ون ٹو فور ٹری سے\n",
    "# LM corrected: میں نے میٹرک کیا ہے\n",
    "# LM corrected: میں نے دو ہزار دس نے میٹرک کیا تھے\n",
    "# LM corrected: میرا پیشہ پلمبر ہے\n",
    "# LM corrected: میں نے دس سال کام کیا ہے"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #################################### Keyword extraction functions ############################################\n",
    "# def get_name(sentence):\n",
    "#   tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "#   name=[]\n",
    "#   for i in range(len(tokens)):\n",
    "#     if (tokens[i]=='میرا'):\n",
    "#       i+=1\n",
    "#       if (tokens[i]=='نام'):\n",
    "#         i+=1\n",
    "#         name=tokens[i:]\n",
    "#         for n in name:\n",
    "#             if (name!=\"ہے\"):\n",
    "#                 x=name_Dist(name)\n",
    "#                 sentence[i]=x\n",
    "#             i+=1\n",
    "#     elif (tokens[i]=='میں'):\n",
    "#       i+=1\n",
    "#       name=tokens[i:]\n",
    "#       for n in name:\n",
    "#             if (name!=\"ہو\" and name!=\"ہوں\" ):\n",
    "#                 x=name_Dist(name)\n",
    "#                 sentence[i]=x \n",
    "# #   str1=\" \"\n",
    "# #   if len(name)==0:\n",
    "# #     str1='None'\n",
    "# #   else:\n",
    "# #     str1=str1.join(sentence)\n",
    "    \n",
    "#   return sentence\n",
    "\n",
    "\n",
    "# def get_age(sentence):\n",
    "#   tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "#   age=[]\n",
    "\n",
    "#   doc=nlp(sentence)\n",
    "#   for X in doc.ents:\n",
    "#     if(X.label_=='DATE'):\n",
    "#       age.append(X.text)\n",
    "\n",
    "#   if(len(age)==0):\n",
    "#     age=['None']\n",
    "    \n",
    "#   str1=\" \"\n",
    "#   if len(age)==0:\n",
    "#     str1='None'\n",
    "#   else:\n",
    "#     str1=str1.join(age)\n",
    "#   return str1\n",
    "\n",
    "# def get_phone(sentence):\n",
    "#   num = \"\"\n",
    "#   for c in sentence:\n",
    "#       if c.isdigit():\n",
    "#           num = num + c\n",
    "#   if num==\"\":\n",
    "#     num='None'\n",
    "#   return num\n",
    "\n",
    "\n",
    "# def get_address(sentence):\n",
    "#   tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "#     #what if users says something diff like mera ghar karachi main hain lekin islamabad main kaam kr raha hun  (\"I live in Rawalpindi\")\n",
    "#     #POS tagger , NER\n",
    "#   addr=[]\n",
    "#   for i in range(len(tokens)):\n",
    "\n",
    "#     #I live in Faisalabad\n",
    "#     if (tokens[i].lower()=='live'):\n",
    "#       i+=1\n",
    "#       if (tokens[i].lower()=='in'):\n",
    "#         i+=1\n",
    "#       addr=tokens[i:]\n",
    "\n",
    "#     #I belong to \n",
    "#     elif (tokens[i].lower()=='belong'):\n",
    "#       i+=1\n",
    "#       if (tokens[i].lower()=='to'):\n",
    "#         i+=1\n",
    "#       addr=tokens[i:]\n",
    "\n",
    "#     #I am from \n",
    "#     elif (tokens[i].lower()=='from'):\n",
    "#       i+=1\n",
    "#       addr=tokens[i:]\n",
    "#     else:\n",
    "#       addr=tokens[i:]\n",
    "\n",
    "#   addr[:2]\n",
    "#   str1=\" \"\n",
    "#   if len(addr)==0:\n",
    "#     str1='None'\n",
    "#   else:\n",
    "#     str1=str1.join(addr)\n",
    "#   return str1\n",
    "\n",
    "\n",
    "# def get_education(sentence):\n",
    "#   year=''\n",
    "#   s=text2int(sentence) +' '     #convert year in words to number                                  \n",
    "#   yr=re.findall(\"(\\d{4})\\D\",s)\n",
    "#   if (len(yr)!=0):      # IF THERE is any year mentioned, get it\n",
    "#     year=yr[0]\n",
    "\n",
    "#   tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "#   EDUCATION = ['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th','11th','12th',\n",
    "#               'first','second','third','fourth','fifth','sixth','seventh','eight','ninth','tenth','elevnth','twelfth',\n",
    "#               'inter','middle','intermediate','matriculation','matric','ICS','Diploma','FSC','FA',\n",
    "#               'BA','BCom','B.Com',  'BTECH', 'B.TECH' , 'graduate', 'SSC', 'HSC' ,'illiterate' ]\n",
    "#   edu=[]\n",
    "#   for i in range(len(tokens)):\n",
    "#     x=tokens[i].lower()\n",
    "#     for i in range(len(EDUCATION)):\n",
    "#       c=EDUCATION[i].lower()\n",
    "#       if (nltk.edit_distance(c , x )<=1):\n",
    "#           if (i<24):\n",
    "#             c+=\" grade\"\n",
    "#           edu.append(c)\n",
    "#           break\n",
    "#   if len(edu)==0:\n",
    "#     str1='None'\n",
    "#   else:\n",
    "#     str1=str(edu[0])\n",
    "#   return str1\n",
    "\n",
    "# def get_year(sentence):\n",
    "#     year=''\n",
    "#     s=text2int(sentence) +' '     #convert year in words to number                                  \n",
    "#     yr=re.findall(\"(\\d{4})\\D\",s)\n",
    "#     if (len(yr)!=0):      # IF THERE is any year mentioned, get it\n",
    "#          year=str(max(yr))\n",
    "\n",
    "#     return year\n",
    "\n",
    "\n",
    "# def get_profession(sentence):\n",
    "#   tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "#   proffession=[\"delivery\",\"carpenter\", \"driver\",\"electrician\",\"plumber\"]\n",
    "#   #\"I know plumber work\"\n",
    "#   #\"I am a carpenter\"\n",
    "#   prof=[]\n",
    "#   for i in range(len(tokens)):\n",
    "#     #I live in Faisalabad\n",
    "#     x=tokens[i].lower()\n",
    "#     for c in proffession:\n",
    "#       if (nltk.edit_distance(c , x )<3):\n",
    "#           if (c==\"delivery\"):\n",
    "#             c=\"delivery boy\"\n",
    "#           prof.append(c)\n",
    "#   str1=\" \"\n",
    "#   if len(prof)==0:\n",
    "#     str1='None'\n",
    "#   else:\n",
    "#     str1=str1.join(prof) \n",
    "#   return str1\n",
    "\n",
    "\n",
    "# def get_experience(line):\n",
    "#   tokens = nltk.tokenize.word_tokenize(line)\n",
    "#   exp=[]\n",
    "#   doc=nlp(line)\n",
    "#   for X in doc.ents:\n",
    "#     if(X.label_=='DATE'):\n",
    "#       exp.append(X.text)\n",
    "\n",
    "#   if(len(exp)==0):\n",
    "#     exp=['None']\n",
    "    \n",
    "#   str1=\" \"\n",
    "#   if len(exp)==0:\n",
    "#     str1='None'\n",
    "#   else:\n",
    "#     str1=str1.join(exp) \n",
    "  \n",
    "#   return str1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ModelCheck.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
